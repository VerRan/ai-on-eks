
# ------------------------------------------------------------------------------
# NIMCache Resource Definition for NVIDIA Inference Microservice (NIM) on EKS
# ------------------------------------------------------------------------------
# This manifest pre-pulls and caches model profiles (optimized for specific GPU types)
# into a shared PersistentVolume (PVC), enabling faster startup times for NIMService pods.
# Use this when deploying large LLMs like LLaMA 3 on multi-GPU nodes to reduce cold start.
# ------------------------------------------------------------------------------

---
apiVersion: apps.nvidia.com/v1alpha1
kind: NIMCache
metadata:
  name: meta-llama3-8b-instruct
  namespace: nim-service
spec:
  source:
    ngc:
      modelPuller: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.3.3
      pullSecret: ngc-secret
      authSecret: ngc-api-secret
      model:
        engine: tensorrt_llm # vllm or tensort or onnx
        # tensorParallelism: "1"
        precision: "fp16"
        # qosProfile: "throughput" # or latency
        gpus:
          - product: "A10G" # L4 for G6
  storage:
    pvc:
      create: true
      storageClass: efs-sc-dynamic
      size: "50Gi"
      volumeAccessMode: ReadWriteMany # You have to use EFS or NFS for ReadWriteMany
  nodeSelector:
    accelerator: "nvidia"
    instanceType: "g5-gpu-karpenter"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources: {}
